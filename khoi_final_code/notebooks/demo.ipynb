{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab9b39b-9030-4b0e-8988-3248c01f5d88",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e66171-2a1a-42c7-a088-cf0ca4ed7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "skvideo.setFFmpegPath(\"/wecare/home/khoi/Downloads/ffmpeg-5.0.1-amd64-static\")\n",
    "import skvideo.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "from os.path import join\n",
    "from utils.data_path import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea4f67-0ab9-42ab-a0cc-801be4a4d833",
   "metadata": {},
   "source": [
    "# Ids range for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cc5d4a-d314-4674-9e69-565669df8bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_range = range(100, 160)\n",
    "IMG_DIR = join(AUGMENTED_SCANS, \"fold_4\")\n",
    "MASK_DIR = join(AUGMENTED_MASKS, \"fold_4\")\n",
    "\n",
    "filtered_ids = sorted([f for f in os.listdir(IMG_DIR) if re.search(f\"CTP10_001_0[1-2][0-9][0-9].png\" ,f)])\n",
    "len(filtered_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d699d20c-7780-4374-9bb6-36ffb2dd9772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTP10_001_0100.png', 'CTP10_001_0101.png', 'CTP10_001_0102.png']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ids[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c760d91-d9b7-400f-aaf9-36d9cdc0fc91",
   "metadata": {},
   "source": [
    "# Create demo video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eefd31-25b3-4cd7-9a07-77cf27f5462c",
   "metadata": {},
   "source": [
    "## Original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe08ea77-dbc2-4d03-b538-c2cdbefb7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_arr = np.zeros((200, 768, 768, 1))\n",
    "for i, img_id in enumerate(filtered_ids):\n",
    "    img_path = Path(IMG_DIR) / img_id\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    vid_arr[i, ...] = img[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa0c97b-dbd5-461d-b69c-c2115c4021b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skvideo.io.vwrite(\"demo/yolov5/original_v1.mp4\", vid_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1138a-8f7c-46fc-b9cc-8003bda91875",
   "metadata": {},
   "source": [
    "## Video made from cropped patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0dda59-b7e9-4a1b-80bd-2d9194894a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROPS_DIR = join(YOLOV5_DETECT, \"holdout_1/fold_4/crops/lesion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448b000f-5196-4300-881d-3da3b48d31b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_crops_id = sorted([f for f in os.listdir(CROPS_DIR) if re.search(f\"CTP10_001_0[1-2][0-9][0-9][0-9]*.jpg\" ,f)])\n",
    "len(all_crops_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddf08c85-0798-4fc0-96e5-a0deeb625f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTP10_001_01342.jpg',\n",
       " 'CTP10_001_0135.jpg',\n",
       " 'CTP10_001_0136.jpg',\n",
       " 'CTP10_001_0137.jpg',\n",
       " 'CTP10_001_0138.jpg',\n",
       " 'CTP10_001_0139.jpg',\n",
       " 'CTP10_001_0140.jpg']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_crops_id[18:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5eb40bf-34fd-47e1-8a84-a76084f1d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_arr = np.zeros((73, 192, 192, 1))\n",
    "for i, img_id in enumerate(all_crops_id):\n",
    "    img_path = Path(CROPS_DIR) / img_id\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    crops_arr[i, ...] = img[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dddabf6-aa34-4803-bbc6-4dc72581ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skvideo.io.vwrite(\"demo/2D_Unet/crops_unsegmented_v1.mp4\", crops_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3f2b8-cce4-43f8-8f14-cef8072d591e",
   "metadata": {},
   "source": [
    "## Video made from cropped patches (segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75bf9428-8154-49ff-b4ae-5fe9badae40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(mask_dir,  x):\n",
    "    \"\"\"\n",
    "    Process the path of the original image. Take out the fold number and image ID. Then process it\n",
    "    to create a proper path for 2D U-Net training, val or testing. Return a pair of image, one for\n",
    "    cropped image and cropped mask.\n",
    "    --------------------\n",
    "    Parameters:\n",
    "    \n",
    "    mask_dir: Directory that contains all the cropped true masks. For example, /wecare/home/khoi/thesis/runs/yolov5/detect/holdout_1_true\n",
    "    x: absolute path to the image\n",
    "    \"\"\"\n",
    "    splits = tf.strings.split(x, sep = os.sep)\n",
    "    fold, img_id = splits[-4], tf.strings.split(splits[-1], sep = \".\")[0]\n",
    "    img_path = x\n",
    "    mask_path = tf.strings.join([tf.constant(mask_dir, dtype = tf.string),fold, \"crops/lesion\",img_id + '.png'], separator = os.sep)\n",
    "\n",
    "    img = decode_png(read_file(img_path), channels = 1)\n",
    "    mask = decode_png(read_file(mask_path), channels = 1)\n",
    "    #img = tf.image.resize(img, [192, 192], method = \"nearest\")\n",
    "    #mask = tf.image.resize(mask, [192, 192], method = \"nearest\")\n",
    "    mask = tf.cast(equal(mask, 255), tf.float32)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dab8d73-57d3-437c-a18d-1300c182aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.io import read_file, decode_png\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "holdout = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebe8fd6f-2e93-4420-aaf6-d189b12b6706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wecare/home/khoi/thesis/runs/yolov5/detect/holdout_1/fold_4/crops/lesion/CTP10_001_0100.jpg',\n",
       " '/wecare/home/khoi/thesis/runs/yolov5/detect/holdout_1/fold_4/crops/lesion/CTP10_001_0101.jpg',\n",
       " '/wecare/home/khoi/thesis/runs/yolov5/detect/holdout_1/fold_4/crops/lesion/CTP10_001_0102.jpg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops_abs_paths = sorted([join(CROPS_DIR, f) for f in os.listdir(CROPS_DIR) if re.search(f\"CTP10_001_01[0-5][0-9][0-9]*.jpg\" ,f)])\n",
    "crops_abs_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1aab326f-8366-48e6-a627-fd3b21e1ccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ds = Dataset.from_tensor_slices(crops_abs_paths)\n",
    "len(demo_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dceae8c-9280-4efd-b827-f90723f7123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ds = demo_ds.map(lambda x: process(str(Path(YOLOV5_DETECT) / f\"holdout_{holdout}_true\"), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3741c-46ae-4f61-85a6-7277b4af951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
